<!DOCTYPE html><html lang="en"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" /><link rel="stylesheet" href="../styles/style.css" /><title>RC week 8: an efficient VM for a functional language</title></head><body><p><a href="..#notes">Notes</a><br />2025/07/10</p><h1>RC week 8: an efficient VM for a functional language</h1><p>Over the past weeks at the <a href="https://www.recurse.com/">Recurse Center</a> I've increasingly been thinking about how to implement an efficient bytecode VM for <a href="https://github.com/fkettelhoit/kombucha">Kombucha</a>, a minimal and malleable programming language for symbiotic end-user programming. Kombucha is a strict, functional language (supporting side effects and effect handlers) that is desugared to an intermediate language similar to lambda calculus.</p><p>I have tried to leave optimizations for later, but it turns out that the semantics of a functional language are in some regards closely tied to the nature of the instruction set of a functional bytecode VM, which is why it makes sense to put some thought into the instruction set before writing large parts of the standard library.</p><h2>Tail call optimization</h2><p>Let's start with a well known example: tail call optimization. Whenever a function call appears in “tail position”, in other words as the last instruction before a <code>RETURN</code> instruction, it is unnecessary to push a new call frame onto the call stack, because the current call frame will be popped by the following <code>RETURN</code> instruction. Instead, the current call frame can be reused as the call frame of the tail call.</p><p>This is not just a useful optimization, but something that functional programmers <em>depend on as part of the semantics</em> of their language. Tail call optimization guarantees that recursion can be used to implement loop-like behavior without running out of stack space, which is a guarantee that every functional language worth its salt must provide.</p><p>Since tail calls are determined <em>statically</em> (just by looking at the syntactic structure of the code), a compiler could emit a special <code>TAIL_CALL</code> instruction. Alternatively, since a tail call is a function call immediately followed by a <code>RETURN</code> instruction, the VM could also check the next instruction after a function call and implement tail call optimization in case the following instruction is a <code>RETURN</code>.</p><h2>Multi-argument functional calls</h2><p>More interestingly, multi-argument function calls pose an interesting problem for functional bytecode VMs, as lambda calculus does not provide “true” multi-argument functions. Instead, functions with multiple arguments are just <a href="https://en.wikipedia.org/wiki/Currying">curried</a> single-argument functions.</p><p>A naive VM implementation would turn multi-argument functions into nested closures, allocating heap space for all function arguments except the last one. This is obviously quite inefficient and it would be desirable to avoid heap allocation in cases where a curried multi-argument function is called with enough argument to “saturate” its arity, because in such a case all arguments can be accessed from the stack.</p><p><a href="https://k-monk.org/">River Dillon Keefer</a> pointed me to Xavier Leroy's work on the ZINC Abstract Machine (<a href="https://xavierleroy.org/mpri/2-4/machines.pdf">slides</a> and <a href="https://inria.hal.science/inria-00070049v1/document">dissertation</a>), which is an instruction set for strict functional languages like ML that avoids heap allocation for the case mentioned above.</p><p>The basic idea is to “mark” how many arguments are applied to a function (ZINC uses a marker value, but it should also be possible to use a stack of arities) so that whenever a function is called, it can tell whether it needs to return a closure or not: If a marker has been reached (meaning there are no more arguments) but the function expects more arguments, it captures its current environment as a closure. If a marker is reached and the function has all its arguments, it can simply return a value without creating a closure. If a marker has not been reached but the function has all its arguments, whatever the function returns will be applied to the remaining arguments.</p><p>Leroy's ZINC (and ML implementations in general) seem to use right-to-left evaluation order of arguments, which is more elegant in terms of implementation but is something that I would like to avoid for Kombucha, as I find left-to-right evaluation order more natural in the presence of side effects. As far as I can tell, it should be possible to adapt the basic idea of ZINC to use left-to-right evaluation order though, if one is willing to accept a slightly less elegant/efficient implementation.</p><h2>Basic data structure accessors</h2><p>The most fundamental and far-reaching decision, however, is which basic data structure to use and how to access and destructure it. Traditionally, functional languages have used cons lists in one form or another (even if they are built out of algebraic data types such as in ML-like languages), because cons lists make it remarkably easy to share the tail of a list, making them the simplest <a href="https://en.wikipedia.org/wiki/Persistent_data_structure">persistent data structure</a>.</p><p>Persistent data structures are great if their sub parts are shared and used in many different places, but they are not especially cache-friendly and are even more wasteful in scenarios where there's just a single owner of the data structure that wants to modify it in-place. In such a scenario, mutable data structures such as (dynamic) arrays fare much better.</p><p>An interesting development in this space is <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2020/11/perceus-tr-v1.pdf">Perceus reference counting</a>, which is a clever way to mutate persistent data structures in place if there's just a single owner. Crucially, what the paper calls the “functional but in place (FBIP)” paradigm is a semantic guarantee just like tail call optimization, which allows programmers to reason about code in a way that lets them know where and when persistent data structures are mutated in place.</p><p>The Perceus algorithm rests on the idea that the problem isn't mutation in itself, it's <em>mutation of shared data</em>. This is the same insight that lies at the root of Rust's borrow checker, which allows either mutation (if there's a single reference) or sharing (if there are multiple borrowed references), but never both at the same time. Perceus mutates a data structure in place if there's a single owner (a refcount of 1), otherwise it behaves like other functional languages and shares the data persistently.</p><p>Because data can be shared persistently, Perceus still assumes the same data model as other functional languages, in other words, cons lists (or other persistent data structures with a lot of pointers and bad cache locality, similar to cons lists).</p><p>An interesting alternative, proposed in the context of imperative languages, are <a href="https://www.jot.fm/issues/issue_2022_02/article2.pdf">Mutable Value Semantics</a>, or MVS for short. MVS guarantee that if there's a single owner of a data structure, it can be mutated in place, similar to Perceus. But MVS and Perceus differ in how they treat the case of multiple references to the same data structure, because instead of sharing it using persistent data structures like Perceus, MVS has copy-on-write semantics where the entire data structure is copied. This might sound wasteful, but makes it possible to use (dynamic) arrays, which have a better cache locality than the persistent data structures used by Perceus.</p><p>I am not aware of any functional language that uses MVS (to be fair, both Perceus and MVS are relatively new approaches), but I see no reason why MVS would be tied to imperative languages. At the end of the day, the difference lies merely in what happens in the case of multiple references to the same data. Whether or not Perceus or MVS is a better option seems to depend on how common it is to mutate data in place instead of sharing it: If single-owner mutation is the common case, MVS is likely to be more efficient, because it can use (dynamic) arrays under the hood and doesn't need to do any pointer chasing. If most data is shared, Perceus is like to be more efficient, because it can avoid copying large chunks of data.</p><p>Going back to the topic of bytecode instruction sets, Perceus and MVS support different instruction sets, because cons lists and (dynamic) arrays support different access methods: While both data structures support pushing a new element to the end of the structure in <code>O(1)</code> and both support accessing the last element in <code>O(1)</code>, only (dynamic) arrays support accessing accessing the <em>first</em> element in <code>O(1)</code> (or any element in the array for that matter).</p><p>An instruction set that can rely on MVS instead of Perceus could thus expose both a <code>LAST</code> <em>and</em> a <code>FIRST</code> instruction, whereas a VM built on persistent data structures doesn't have that luxury, which leads to the classic pattern of having to <code>reverse</code> a cons list after accumulating it using tail calls.</p><h2>Fold considered harmful?</h2><p>It is still unclear to me whether building a VM for a functional language using MVS is actually a good idea. Such an idea might turn out to involve too much copying, whereas using Perceus seems to be the “safer” choice, as it has been used in <a href="https://koka-lang.github.io/koka/doc/index.html">Koka</a> and <a href="https://www.roc-lang.org/">Roc</a> with some success (and appears to be a strict improvement over the usual functional model of using persistent data structures everywhere).</p><p>However, one thing I dislike about using cons lists is that especially in a Lisp-like language such as Kombucha, where algebraic data types are built out of a basic list-like data structure, the programming pattern that emerges is very dependent on <code>fold</code>, because all data is accessed in terms of <code>first</code> and <code>rest</code> (to borrow Clojure's names for <code>car</code> and <code>cdr</code>). As Guy Steele points out, this is a <a href="https://vimeo.com/6624203">fundamentally sequential</a> approach to programming that results in data structures with a long spine (which is also bad for cache locality).</p><p>What I find appealing about MVS is that the basic data structure could be (dynamic) arrays. Could that lead to a working instruction set for a functional bytecode VM? Well, let's see!</p></body></html>